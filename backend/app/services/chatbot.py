"""
ì±—ë´‡ ë©”ì¸ ë¡œì§ - ëª¨ë“  ì„œë¹„ìŠ¤ í†µí•©
"""
from typing import Dict, Any, Optional, List
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from app.config import settings
from app.models.schemas import UserProfile, ChatMessage, CourseInput
from app.services.query_router import query_router
from app.services.vector_service import get_vector_service
from app.services.curriculum_service import curriculum_service
from app.services.entity_extractor import entity_extractor


class SchoolChatbot:
    """í•™êµ ì±—ë´‡ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # LLM ì´ˆê¸°í™” (lazy loading)
        self.llm = None
        self.vector_service = get_vector_service()
    
    def _get_llm(self) -> ChatOpenAI:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
        if self.llm is None:
            self.llm = ChatOpenAI(
                model=settings.model_name,
                temperature=settings.temperature,
                max_tokens=settings.max_tokens,
                openai_api_key=settings.openai_api_key
            )
        return self.llm
    
    def chat(
        self,
        message: str,
        user_profile: Optional[UserProfile] = None,
        history: List[ChatMessage] = None
    ) -> Dict[str, Any]:
        """ë©”ì¸ ì±—ë´‡ ë¡œì§"""
        
        # 1. ì§ˆë¬¸ ë¶„ë¥˜
        query_type = query_router.classify(message)
        needs_profile = query_router.needs_user_profile(message)
        
        print(f"\nğŸ” ë””ë²„ê¹…:")
        print(f"  ë©”ì‹œì§€: {message}")
        print(f"  query_type: {query_type}")
        print(f"  user_profile: {user_profile}")
        print(f"  needs_profile: {needs_profile}")
        
        if query_type == "curriculum":
            extracted = entity_extractor.extract_course_info(message)
            print(f"  extracted: {extracted}")
            
            # ì •ë³´ê°€ ì¶©ë¶„í•˜ë©´ UserProfile ìƒì„±
            if extracted['has_enough_info']:
                user_profile = UserProfile(
                    admission_year=extracted['admission_year'],
                    courses_taken=extracted['courses']
                )
                
                print(f"âœ… UserProfile ìë™ ìƒì„±: {extracted['admission_year']}í•™ë²ˆ, {len(extracted['courses'])}ê³¼ëª©")
            
            # ì—¬ì „íˆ ì •ë³´ ë¶€ì¡±í•˜ë©´ ì•ˆë‚´
            elif not user_profile:
                missing = []
                if not extracted['admission_year']:
                    missing.append("ì…í•™ë…„ë„")
                if not extracted['course_codes']:
                    missing.append("ì´ìˆ˜ ê³¼ëª©")
                
                return {
                    "message": f"""ê°œì¸ ë§ì¶¤ ë‹µë³€ì„ ìœ„í•´ ì •ë³´ê°€ í•„ìš”í•´ìš”! ğŸ˜Š

                    ë‹¤ìŒ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”:
                    {'ğŸ“… ì…í•™ë…„ë„ (ì˜ˆ: 2024í•™ë²ˆ)' if 'ì…í•™ë…„ë„' in missing else ''}
                    {'ğŸ“š ì´ìˆ˜í•œ ê³¼ëª© ì½”ë“œ (ì˜ˆ: CS0614, XG0800)' if 'ì´ìˆ˜ ê³¼ëª©' in missing else ''}

                    ì˜ˆì‹œ: "2024í•™ë²ˆì´ê³  CS0614, XG0800 ë“¤ì—ˆì–´"
                    """,
                                        "query_type": query_type,
                                        "sources": [],
                                        "needs_profile": True
                }
                
            return self._handle_curriculum_query(message, user_profile)
        
        # 3. ì§ˆë¬¸ ìœ í˜•ë³„ ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œ)      
        elif query_type == "general":
            return self._handle_general_query(message)
        
        else:  # hybrid
            return self._handle_hybrid_query(message, user_profile)
            
    
    def _handle_general_query(self, message: str) -> Dict[str, Any]:
        """ì¼ë°˜ ì •ë³´ ì§ˆë¬¸ ì²˜ë¦¬ (ë²¡í„° ê²€ìƒ‰)"""
        # ë²¡í„° ê²€ìƒ‰
        search_results = self.vector_service.search(message, k=3)
        
        if not search_results:
            return {
                "message": "ì£„ì†¡í•´ìš”, ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”? ğŸ¤”",
                "query_type": "general",
                "sources": [],
                "needs_profile": False
            }
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        context = self.vector_service.format_search_results(search_results)
        
        # LLM í”„ë¡¬í”„íŠ¸
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ìˆœì²œëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

            ë‹µë³€ ê·œì¹™:
            1. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
            2. ì£¼ì–´ì§„ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”
            3. ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”
            4. í•„ìš”ì‹œ ì´ëª¨ì§€ë¥¼ í™œìš©í•´ ì¹œê·¼í•¨ì„ ë”í•˜ì„¸ìš”

            ê²€ìƒ‰ëœ ì •ë³´:
            {context}
            """),
            ("user", "{question}")
        ])
        
        # LLM í˜¸ì¶œ
        llm = self._get_llm()
        chain = prompt | llm
        
        try:
            response = chain.invoke({
                "context": context,
                "question": message
            })
            
            answer = response.content
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            answer = "ì£„ì†¡í•´ìš”, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ˜…"
        
        return {
            "message": answer,
            "query_type": "general",
            "sources": search_results,
            "needs_profile": False
        }
    
    def _handle_curriculum_query(
        self, 
        message: str, 
        user_profile: UserProfile
    ) -> Dict[str, Any]:
        """êµìœ¡ê³¼ì • ì§ˆë¬¸ ì²˜ë¦¬ (í•˜ì´ë¸Œë¦¬ë“œ)"""
        
        # 1. ë‚¨ì€ í•™ì  ê³„ì‚°
        calculation = curriculum_service.calculate_remaining_credits(user_profile)
        
        if 'error' in calculation:
            return {
                "message": calculation['error'],
                "query_type": "curriculum",
                "sources": [],
                "needs_profile": False
            }
        
        # 2. í¬ë§·íŒ…ëœ ê²°ê³¼
        formatted_info = curriculum_service.format_curriculum_info(calculation)
        
        # 3. ê°„ë‹¨í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨
        simple_keywords = [
            'ë‚¨ì€', 'ì–¼ë§ˆ', 'ëª‡', 'ì§„í–‰', 'í˜„í™©', 'í•™ì ', 'ìƒíƒœ',
            'ë‚¨ì•˜', 'ì´ìˆ˜', 'ë“¤ì—ˆ', 'ì™„ë£Œ'
        ]
        
        is_simple = any(kw in message for kw in simple_keywords)
        
        # ë³µì¡í•œ ì§ˆë¬¸ í‚¤ì›Œë“œ
        complex_keywords = [
            'ì¡¸ì—…', 'ê°€ëŠ¥', 'ì¶”ì²œ', 'ì–´ë–¤', 'ë¬´ì—‡', 'ë­',
            'í•´ì•¼', 'í•˜ë©´', 'ì¢‹', 'ì¡°ì–¸', 'ë„ì›€'
        ]
        
        is_complex = any(kw in message for kw in complex_keywords)
        
        # 4-A. ê°„ë‹¨í•œ ì§ˆë¬¸ â†’ í¬ë§·íŒ…ë§Œ ë°˜í™˜
        if is_simple and not is_complex:
            print("  â†’ ê°„ë‹¨í•œ ì§ˆë¬¸: í¬ë§·íŒ…ë§Œ ë°˜í™˜")
            
            # ë¯¸ì´ìˆ˜ ê³¼ëª© ì¶”ê°€ (ìš”ì²­ ì‹œ)
            additional_info = ""
            if any(kw in message for kw in ['ê³¼ëª©', 'ë­', 'ì–´ë–¤']):
                not_taken = curriculum_service.get_required_courses_not_taken(
                    user_profile,
                    course_area="ì „ê³µ",
                    requirement_type="ì „ê³µí•„ìˆ˜"
                )
                
                if not_taken:
                    formatted_not_taken = curriculum_service.format_not_taken_courses(not_taken)
                    additional_info = "\n\n" + formatted_not_taken
            
            return {
                "message": formatted_info + additional_info,
                "query_type": "curriculum",
                "sources": [],
                "needs_profile": False
            }
        
        # 4-B. ë³µì¡í•œ ì§ˆë¬¸ â†’ LLM ì‚¬ìš©
        print("  â†’ ë³µì¡í•œ ì§ˆë¬¸: LLM ì‚¬ìš©")
        
        # ë¯¸ì´ìˆ˜ ê³¼ëª© ì •ë³´
        not_taken_info = ""
        if any(kw in message for kw in ['ê³¼ëª©', 'ë¬´ì—‡', 'ë­', 'ì–´ë–¤', 'ë‚¨ì•˜', 'í•„ìˆ˜']):
            # ì§ˆë¬¸ì—ì„œ ì˜ì—­ ì¶”ì¶œ
            area = None
            req_type = None
            
            if 'ì „ê³µí•„ìˆ˜' in message:
                area = "ì „ê³µ"
                req_type = "ì „ê³µí•„ìˆ˜"
            elif 'ì „ê³µì„ íƒ' in message:
                area = "ì „ê³µ"
                req_type = "ì „ê³µì„ íƒ"
            elif 'êµì–‘' in message:
                area = "êµì–‘"
            
            not_taken = curriculum_service.get_required_courses_not_taken(
                user_profile,
                course_area=area,
                requirement_type=req_type
            )
            
            if not_taken:
                not_taken_info = "\n\nğŸ“š ë¯¸ì´ìˆ˜ ê³¼ëª©:\n"
                for course in not_taken[:10]:  # ìµœëŒ€ 10ê°œ
                    not_taken_info += f"- {course['course_name']} ({course['credit']}í•™ì )"
                    if course.get('recommended_semester'):
                        not_taken_info += f" [ê¶Œì¥: {course['grade']}í•™ë…„ {course['semester']}í•™ê¸°]"
                    not_taken_info += "\n"
        
        # LLM í”„ë¡¬í”„íŠ¸
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ìˆœì²œëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ í•™ì‚¬ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.

    ì£¼ì–´ì§„ ì¡¸ì—…ìš”ê±´ í˜„í™©ì„ **ì •í™•íˆ** ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    âš ï¸ ì¤‘ìš”:
    1. ì•„ë˜ ì¡¸ì—…ìš”ê±´ í˜„í™©ì˜ ìˆ«ìë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
    2. ì„ì˜ë¡œ ìˆ«ìë¥¼ ë§Œë“¤ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
    3. í•™ìƒì„ ê²©ë ¤í•˜ê³  ì‘ì›í•˜ëŠ” í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
    4. êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”

    ì¡¸ì—…ìš”ê±´ í˜„í™©:
    {formatted_info}

    {not_taken_info}
    """),
            ("user", "{question}")
        ])
        
        llm = self._get_llm()
        chain = prompt | llm
        
        try:
            response = chain.invoke({
                "formatted_info": formatted_info,
                "not_taken_info": not_taken_info,
                "question": message
            })
            answer = response.content
            
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # LLM ì‹¤íŒ¨ ì‹œ í¬ë§·íŒ… ê²°ê³¼ ë°˜í™˜
            answer = formatted_info + not_taken_info
        
        return {
            "message": answer,
            "query_type": "curriculum",
            "sources": [],
            "needs_profile": False
        }

    def _handle_hybrid_query(
        self,
        message: str,
        user_profile: Optional[UserProfile]
    ) -> Dict[str, Any]:
        """ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬ (ë²¡í„° + SQL)"""
        
        # 1. ë²¡í„° ê²€ìƒ‰
        search_results = self.vector_service.search(message, k=2)
        context = self.vector_service.format_search_results(search_results)
        
        # 2. êµìœ¡ê³¼ì • ì •ë³´ (í”„ë¡œí•„ì´ ìˆìœ¼ë©´)
        curriculum_info = ""
        if user_profile:
            calculation = curriculum_service.calculate_remaining_credits(user_profile)
            if 'error' not in calculation:
                curriculum_info = curriculum_service.format_curriculum_info(calculation)
        
        # 3. LLMìœ¼ë¡œ í†µí•© ë‹µë³€
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ìˆœì²œëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ì¢…í•© ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
            í•™êµ ì •ë³´ì™€ í•™ìƒì˜ ê°œì¸ í•™ì‚¬ ì •ë³´ë¥¼ ì¢…í•©í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

            í•™êµ ì •ë³´:
            {context}

            {curriculum_section}

            ë‹µë³€ ê·œì¹™:
            1. ë‘ ê°€ì§€ ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•´ ë‹µë³€í•˜ì„¸ìš”
            2. í•™ìƒì˜ ìƒí™©ì— ë§ëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
            3. ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
            """),
                    ("user", "{question}")
        ])
        
        curriculum_section = ""
        if curriculum_info:
            curriculum_section = f"í•™ìƒ ì¡¸ì—…ìš”ê±´ í˜„í™©:\n{curriculum_info}"
        
        llm = self._get_llm()
        chain = prompt | llm
        
        try:
            response = chain.invoke({
                "context": context,
                "curriculum_section": curriculum_section,
                "question": message
            })
            answer = response.content
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            answer = f"{context}\n\n{curriculum_info}" if curriculum_info else context
        
        return {
            "message": answer,
            "query_type": "hybrid",
            "sources": search_results,
            "needs_profile": False
        }
chatbot = SchoolChatbot()