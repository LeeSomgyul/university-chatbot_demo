"""
ì±—ë´‡ ë©”ì¸ ë¡œì§ - ëª¨ë“  ì„œë¹„ìŠ¤ í†µí•©
"""
from typing import Dict, Any, Optional, List
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from app.config import settings
from app.models.schemas import UserProfile, ChatMessage
from app.services.query_router import query_router
from app.services.vector_service import get_vector_service
from app.services.curriculum_service import curriculum_service


class SchoolChatbot:
    """í•™êµ ì±—ë´‡ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # LLM ì´ˆê¸°í™” (lazy loading)
        self.llm = None
        self.vector_service = get_vector_service()
    
    def _get_llm(self) -> ChatOpenAI:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
        if self.llm is None:
            self.llm = ChatOpenAI(
                model=settings.model_name,
                temperature=settings.temperature,
                max_tokens=settings.max_tokens,
                openai_api_key=settings.openai_api_key
            )
        return self.llm
    
    def chat(
        self,
        message: str,
        user_profile: Optional[UserProfile] = None,
        history: List[ChatMessage] = None
    ) -> Dict[str, Any]:
        """
        ë©”ì¸ ì±—ë´‡ ë¡œì§
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ (í•™ë²ˆ, ìˆ˜ê°•ì´ë ¥ ë“±)
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬
        
        Returns:
            {
                "message": "ë‹µë³€",
                "query_type": "general|curriculum|hybrid",
                "sources": [...],
                "needs_profile": False
            }
        """
        # 1. ì§ˆë¬¸ ë¶„ë¥˜
        query_type = query_router.classify(message)
        needs_profile = query_router.needs_user_profile(message)
        
        # 2. ì‚¬ìš©ì í”„ë¡œí•„ í•„ìš”í•œë° ì—†ìœ¼ë©´ ìš”ì²­
        if needs_profile and not user_profile:
            return {
                "message": "ê°œì¸ ë§ì¶¤ ë‹µë³€ì„ ìœ„í•´ í•™ë²ˆê³¼ ìˆ˜ê°• ì´ë ¥ì„ ë¨¼ì € ì•Œë ¤ì£¼ì„¸ìš”. ğŸ˜Š",
                "query_type": query_type,
                "sources": [],
                "needs_profile": True
            }
        
        # 3. ì§ˆë¬¸ ìœ í˜•ë³„ ì²˜ë¦¬
        if query_type == "curriculum":
            return self._handle_curriculum_query(message, user_profile)
        
        elif query_type == "general":
            return self._handle_general_query(message)
        
        else:  # hybrid
            return self._handle_hybrid_query(message, user_profile)
    
    def _handle_general_query(self, message: str) -> Dict[str, Any]:
        """ì¼ë°˜ ì •ë³´ ì§ˆë¬¸ ì²˜ë¦¬ (ë²¡í„° ê²€ìƒ‰)"""
        # ë²¡í„° ê²€ìƒ‰
        search_results = self.vector_service.search(message, k=3)
        
        if not search_results:
            return {
                "message": "ì£„ì†¡í•´ìš”, ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”? ğŸ¤”",
                "query_type": "general",
                "sources": [],
                "needs_profile": False
            }
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        context = self.vector_service.format_search_results(search_results)
        
        # LLM í”„ë¡¬í”„íŠ¸
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ìˆœì²œëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

            ë‹µë³€ ê·œì¹™:
            1. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
            2. ì£¼ì–´ì§„ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”
            3. ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”
            4. í•„ìš”ì‹œ ì´ëª¨ì§€ë¥¼ í™œìš©í•´ ì¹œê·¼í•¨ì„ ë”í•˜ì„¸ìš”

            ê²€ìƒ‰ëœ ì •ë³´:
            {context}
            """),
            ("user", "{question}")
        ])
        
        # LLM í˜¸ì¶œ
        llm = self._get_llm()
        chain = prompt | llm
        
        try:
            response = chain.invoke({
                "context": context,
                "question": message
            })
            
            answer = response.content
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            answer = "ì£„ì†¡í•´ìš”, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ğŸ˜…"
        
        return {
            "message": answer,
            "query_type": "general",
            "sources": search_results,
            "needs_profile": False
        }
    
    def _handle_curriculum_query(
        self, 
        message: str, 
        user_profile: UserProfile
    ) -> Dict[str, Any]:
        """êµìœ¡ê³¼ì • ì§ˆë¬¸ ì²˜ë¦¬ (SQL ê³„ì‚°)"""
        
        # ë‚¨ì€ í•™ì  ê³„ì‚°
        calculation = curriculum_service.calculate_remaining_credits(user_profile)
        
        if 'error' in calculation:
            return {
                "message": calculation['error'],
                "query_type": "curriculum",
                "sources": [],
                "needs_profile": False
            }
        
        # ê³„ì‚° ê²°ê³¼ í¬ë§·íŒ…
        curriculum_info = curriculum_service.format_curriculum_info(calculation)
        
        # íŠ¹ì • ì˜ì—­ ê³¼ëª© ëª©ë¡ì´ í•„ìš”í•œì§€ íŒë‹¨
        course_list = ""
        if any(kw in message for kw in ['ê³¼ëª©', 'ë¬´ì—‡', 'ë­', 'ì–´ë–¤', 'ë‚¨ì•˜']):
            # ì§ˆë¬¸ì—ì„œ ì˜ì—­ ì¶”ì¶œ ì‹œë„
            area = None
            if 'ì „ê³µí•„ìˆ˜' in message:
                area = 'ì „ê³µí•„ìˆ˜'
            elif 'ì „ê³µì„ íƒ' in message:
                area = 'ì „ê³µì„ íƒ'
            elif 'êµì–‘' in message:
                area = 'êµì–‘í•„ìˆ˜'
            
            if area:
                not_taken = curriculum_service.get_courses_not_taken(user_profile, area)
                if not_taken:
                    course_list = f"\n\nğŸ“š ì•„ì§ ì•ˆ ë“¤ì€ {area} ê³¼ëª©:\n"
                    for course in not_taken[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                        course_list += f"- {course['course_name']} ({course['credit']}í•™ì )\n"
        
        # LLMìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ìˆœì²œëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ í•™ì‚¬ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
            í•™ìƒì˜ ì¡¸ì—…ìš”ê±´ê³¼ ìˆ˜ê°•ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

            ë‹µë³€ ê·œì¹™:
            1. ê³„ì‚°ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
            2. í•™ìƒì„ ê²©ë ¤í•˜ê³  ì‘ì›í•˜ëŠ” í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
            3. êµ¬ì²´ì ì¸ ìˆ«ìì™€ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
            4. í•„ìš”ì‹œ ë‹¤ìŒ ìˆ˜ê°• ì¶”ì²œë„ í•´ì£¼ì„¸ìš”

            í•™ìƒ ì •ë³´:
            - í•™ë²ˆ: {admission_year}
            - íŠ¸ë™: {track}

            ì¡¸ì—…ìš”ê±´ í˜„í™©:
            {curriculum_info}

            {course_list}
            """),
            ("user", "{question}")
        ])
        
        llm = self._get_llm()
        chain = prompt | llm
        
        try:
            response = chain.invoke({
                "admission_year": user_profile.admission_year,
                "track": user_profile.track,
                "curriculum_info": curriculum_info,
                "course_list": course_list,
                "question": message
            })
            answer = response.content
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # LLM ì‹¤íŒ¨ ì‹œ ê³„ì‚° ê²°ê³¼ë§Œ ë°˜í™˜
            
    # app/services/chatbot.pyì—ì„œ í™•ì¸

    def _handle_hybrid_query(
        self,
        message: str,
        user_profile: Optional[UserProfile]
    ) -> Dict[str, Any]:
        """ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬ (ë²¡í„° + SQL)"""
        
        # 1. ë²¡í„° ê²€ìƒ‰
        search_results = self.vector_service.search(message, k=2)
        context = self.vector_service.format_search_results(search_results)
        
        # 2. êµìœ¡ê³¼ì • ì •ë³´ (í”„ë¡œí•„ì´ ìˆìœ¼ë©´)
        curriculum_info = ""
        if user_profile:
            calculation = curriculum_service.calculate_remaining_credits(user_profile)
            if 'error' not in calculation:
                curriculum_info = curriculum_service.format_curriculum_info(calculation)
        
        # 3. LLMìœ¼ë¡œ í†µí•© ë‹µë³€
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ìˆœì²œëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ì¢…í•© ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
            í•™êµ ì •ë³´ì™€ í•™ìƒì˜ ê°œì¸ í•™ì‚¬ ì •ë³´ë¥¼ ì¢…í•©í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.

            í•™êµ ì •ë³´:
            {context}

            {curriculum_section}

            ë‹µë³€ ê·œì¹™:
            1. ë‘ ê°€ì§€ ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•´ ë‹µë³€í•˜ì„¸ìš”
            2. í•™ìƒì˜ ìƒí™©ì— ë§ëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
            3. ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
            """),
                    ("user", "{question}")
        ])
        
        curriculum_section = ""
        if curriculum_info:
            curriculum_section = f"í•™ìƒ ì¡¸ì—…ìš”ê±´ í˜„í™©:\n{curriculum_info}"
        
        llm = self._get_llm()
        chain = prompt | llm
        
        try:
            response = chain.invoke({
                "context": context,
                "curriculum_section": curriculum_section,
                "question": message
            })
            answer = response.content
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            answer = f"{context}\n\n{curriculum_info}" if curriculum_info else context
        
        return {
            "message": answer,
            "query_type": "hybrid",
            "sources": search_results,
            "needs_profile": False
        }
chatbot = SchoolChatbot()